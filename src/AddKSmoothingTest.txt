Add-k Smoothing Training:
k = 0.1, pp1 = 294.7609386516644, pp2 = 2317.3964357076393.
k = 0.2, pp1 = 294.6052223726746, pp2 = 2079.014584948673.
k = 0.30000000000000004, pp1 = 294.49297964752463, pp2 = 1999.7045758874972.
k = 0.4, pp1 = 294.4186495870981, pp2 = 1965.5402367097088.
k = 0.5, pp1 = 294.3777328596813, pp2 = 1950.254845330444.
k = 0.6, pp1 = 294.36652850092327, pp2 = 1944.4293126874807.
k = 0.7000000000000001, pp1 = 294.3819488950896, pp2 = 1943.8612536018506.
k = 0.8, pp1 = 294.42138638804335, pp2 = 1946.4084904522797.
k = 0.9, pp1 = 294.4826149895663, pp2 = 1950.8762348917971.
k = 1.0, pp1 = 294.56371652769536, pp2 = 1956.553709945939.
Using truthful language model on truthful validation set:
The minimun unigram perplexity is 294.36652850092327, k = 0.6.
The minimun bigram perplexity is 1943.8612536018506, k = 0.7000000000000001.

k = 0.1, pp1 = 285.7836677202058, pp2 = 2562.264888230758.
k = 0.2, pp1 = 285.679693543241, pp2 = 2269.724665363147.
k = 0.30000000000000004, pp1 = 285.6153165984405, pp2 = 2167.958437586972.
k = 0.4, pp1 = 285.58547516509293, pp2 = 2120.9374574375174.
k = 0.5, pp1 = 285.5860733532434, pp2 = 2097.1044206367815.
k = 0.6, pp1 = 285.6137417309249, pp2 = 2085.0696057088617.
k = 0.7000000000000001, pp1 = 285.6656690251524, pp2 = 2079.7157996647193.
k = 0.8, pp1 = 285.7394807740482, pp2 = 2078.414290290913.
k = 0.9, pp1 = 285.83314989122687, pp2 = 2079.686191672627.
k = 1.0, pp1 = 285.9449294712449, pp2 = 2082.6430493912712.
Using truthful language model on deceptive validation set:
The minimun unigram perplexity is 285.58547516509293, k = 0.4.
The minimun bigram perplexity is 2078.414290290913, k = 0.8.

k = 0.1, pp1 = 266.44996401069955, pp2 = 2543.577629450212.
k = 0.2, pp1 = 266.34582067444813, pp2 = 2206.291218807705.
k = 0.30000000000000004, pp1 = 266.27663608539206, pp2 = 2081.8566928312566.
k = 0.4, pp1 = 266.23791808797534, pp2 = 2019.5498347031548.
k = 0.5, pp1 = 266.2260356319224, pp2 = 1984.1451736135944.
k = 0.6, pp1 = 266.2380047634463, pp2 = 1962.78711685851.
k = 0.7000000000000001, pp1 = 266.27133826429866, pp2 = 1949.6231028591346.
k = 0.8, pp1 = 266.323937313187, pp2 = 1941.601739202741.
k = 0.9, pp1 = 266.3940116933472, pp2 = 1936.9780930933784.
k = 1.0, pp1 = 266.4800198882088, pp2 = 1934.6850368888265.
Using deceptive language model on deceptive validation set:
The minimun unigram perplexity is 266.2260356319224, k = 0.5.
The minimun bigram perplexity is 1934.6850368888265, k = 1.0.

k = 0.1, pp1 = 275.97507101291416, pp2 = 2142.872434989487.
k = 0.2, pp1 = 275.7394178834238, pp2 = 1891.416012437838.
k = 0.30000000000000004, pp1 = 275.5463043068984, pp2 = 1801.2035264416704.
k = 0.4, pp1 = 275.3903003125966, pp2 = 1758.0064158966613.
k = 0.5, pp1 = 275.2670127578844, pp2 = 1735.069185793432.
k = 0.6, pp1 = 275.1728283267174, pp2 = 1722.6316824040039.
k = 0.7000000000000001, pp1 = 275.1047328352072, pp2 = 1716.2583717681593.
k = 0.8, pp1 = 275.060180943794, pp2 = 1713.6429816131376.
k = 0.9, pp1 = 275.03700013075616, pp2 = 1713.4714220870285.
k = 1.0, pp1 = 275.03331854823205, pp2 = 1714.9436357102688.
Using deceptive language model on truthful validation set:
The minimun unigram perplexity is 275.03331854823205, k = 1.0.
The minimun bigram perplexity is 1713.4714220870285, k = 0.9.


Linear Interpolation Smoothing Training:
lambda = 0.1, pp1 = 294.967136456961, pp2 = 264.14031213179464.
lambda = 0.2, pp1 = 294.967136456961, pp2 = 276.0380216189689.
lambda = 0.30000000000000004, pp1 = 294.967136456961, pp2 = 297.47383995948366.
lambda = 0.4, pp1 = 294.967136456961, pp2 = 328.67695375345465.
lambda = 0.5, pp1 = 294.967136456961, pp2 = 373.25213907918396.
lambda = 0.6, pp1 = 294.967136456961, pp2 = 439.24105088058644.
lambda = 0.7000000000000001, pp1 = 294.967136456961, pp2 = 545.0284343710301.
lambda = 0.8, pp1 = 294.967136456961, pp2 = 742.2795039792828.
lambda = 0.9, pp1 = 294.967136456961, pp2 = 1260.1949627412869.
Using truthful language model on truthful validation set:
The minimun unigram perplexity is 294.967136456961, lambda = 0.1.
The minimun bigram perplexity is 264.14031213179464, lambda = 0.1.

lambda = 0.1, pp1 = 285.9336172660487, pp2 = 261.1054096812059.
lambda = 0.2, pp1 = 285.9336172660487, pp2 = 274.8199200945816.
lambda = 0.30000000000000004, pp1 = 285.9336172660487, pp2 = 297.74146680873645.
lambda = 0.4, pp1 = 285.9336172660487, pp2 = 330.5075658505705.
lambda = 0.5, pp1 = 285.9336172660487, pp2 = 377.0089913813493.
lambda = 0.6, pp1 = 285.9336172660487, pp2 = 445.70604253020116.
lambda = 0.7000000000000001, pp1 = 285.9336172660487, pp2 = 555.8827787826144.
lambda = 0.8, pp1 = 285.9336172660487, pp2 = 761.8424680462142.
lambda = 0.9, pp1 = 285.9336172660487, pp2 = 1305.7145269662112.
Using truthful language model on deceptive validation set:
The minimun unigram perplexity is 285.9336172660487, lambda = 0.1.
The minimun bigram perplexity is 261.1054096812059, lambda = 0.1.

lambda = 0.1, pp1 = 266.594732995653, pp2 = 249.71996918161867.
lambda = 0.2, pp1 = 266.594732995653, pp2 = 265.10606936056865.
lambda = 0.30000000000000004, pp1 = 266.594732995653, pp2 = 288.9763273553362.
lambda = 0.4, pp1 = 266.594732995653, pp2 = 322.4190490238943.
lambda = 0.5, pp1 = 266.594732995653, pp2 = 369.51677783103446.
lambda = 0.6, pp1 = 266.594732995653, pp2 = 438.90770798120735.
lambda = 0.7000000000000001, pp1 = 266.594732995653, pp2 = 550.2144154784309.
lambda = 0.8, pp1 = 266.594732995653, pp2 = 758.8075862151221.
lambda = 0.9, pp1 = 266.594732995653, pp2 = 1312.7870422937158.
Using deceptive language model on deceptive validation set:
The minimun unigram perplexity is 266.594732995653, lambda = 0.1.
The minimun bigram perplexity is 249.71996918161867, lambda = 0.1.

lambda = 0.1, pp1 = 276.26010688526037, pp2 = 251.54357322195136.
lambda = 0.2, pp1 = 276.26010688526037, pp2 = 264.1693929743942.
lambda = 0.30000000000000004, pp1 = 276.26010688526037, pp2 = 285.56103404327075.
lambda = 0.4, pp1 = 276.26010688526037, pp2 = 316.222780289296.
lambda = 0.5, pp1 = 276.26010688526037, pp2 = 359.74218972649805.
lambda = 0.6, pp1 = 276.26010688526037, pp2 = 423.954119067455.
lambda = 0.7000000000000001, pp1 = 276.26010688526037, pp2 = 526.691324417171.
lambda = 0.8, pp1 = 276.26010688526037, pp2 = 717.9943086565505.
lambda = 0.9, pp1 = 276.26010688526037, pp2 = 1219.5820228498897.
Using deceptive language model on truthful validation set:
The minimun unigram perplexity is 276.26010688526037, lambda = 0.1.
The minimun bigram perplexity is 251.54357322195136, lambda = 0.1.

lambda = 0.01, pp1 = 294.967136456961, pp2 = 274.02563122564806.
lambda = 0.020000000000000004, pp1 = 294.967136456961, pp2 = 269.36693815566974.
lambda = 0.030000000000000006, pp1 = 294.967136456961, pp2 = 266.7018167485685.
lambda = 0.04000000000000001, pp1 = 294.967136456961, pp2 = 265.0571714466124.
lambda = 0.05000000000000001, pp1 = 294.967136456961, pp2 = 264.05622957433565.
lambda = 0.06000000000000001, pp1 = 294.967136456961, pp2 = 263.5085013352982.
lambda = 0.07, pp1 = 294.967136456961, pp2 = 263.3030834380069.
lambda = 0.08, pp1 = 294.967136456961, pp2 = 263.36940642773675.
lambda = 0.09000000000000001, pp1 = 294.967136456961, pp2 = 263.65973394774903.
lambda = 0.1, pp1 = 294.967136456961, pp2 = 264.14031213179464.
Using truthful language model on truthful validation set:
The minimun unigram perplexity is 294.967136456961, lambda = 0.01.
The minimun bigram perplexity is 263.3030834380069, lambda = 0.07.

lambda = 0.01, pp1 = 285.9336172660487, pp2 = 267.1948294896295.
lambda = 0.020000000000000004, pp1 = 285.9336172660487, pp2 = 263.39194236796783.
lambda = 0.030000000000000006, pp1 = 285.9336172660487, pp2 = 261.34087546204375.
lambda = 0.04000000000000001, pp1 = 285.9336172660487, pp2 = 260.1826670610183.
lambda = 0.05000000000000001, pp1 = 285.9336172660487, pp2 = 259.5879351699078.
lambda = 0.06000000000000001, pp1 = 285.9336172660487, pp2 = 259.39111615054884.
lambda = 0.07, pp1 = 285.9336172660487, pp2 = 259.49620702755004.
lambda = 0.08, pp1 = 285.9336172660487, pp2 = 259.8423145307912.
lambda = 0.09000000000000001, pp1 = 285.9336172660487, pp2 = 260.3883670119402.
lambda = 0.1, pp1 = 285.9336172660487, pp2 = 261.1054096812059.
Using truthful language model on deceptive validation set:
The minimun unigram perplexity is 285.9336172660487, lambda = 0.01.
The minimun bigram perplexity is 259.39111615054884, lambda = 0.06000000000000001.

lambda = 0.01, pp1 = 266.594732995653, pp2 = 251.09056395198184.
lambda = 0.020000000000000004, pp1 = 266.594732995653, pp2 = 248.4587742958693.
lambda = 0.030000000000000006, pp1 = 266.594732995653, pp2 = 247.20736854950016.
lambda = 0.04000000000000001, pp1 = 266.594732995653, pp2 = 246.66244447472567.
lambda = 0.05000000000000001, pp1 = 266.594732995653, pp2 = 246.56608232825454.
lambda = 0.06000000000000001, pp1 = 266.594732995653, pp2 = 246.78886156611966.
lambda = 0.07, pp1 = 266.594732995653, pp2 = 247.25587946941687.
lambda = 0.08, pp1 = 266.594732995653, pp2 = 247.91973729251612.
lambda = 0.09000000000000001, pp1 = 266.594732995653, pp2 = 248.74856128193483.
lambda = 0.1, pp1 = 266.594732995653, pp2 = 249.71996918161867.
Using deceptive language model on deceptive validation set:
The minimun unigram perplexity is 266.594732995653, lambda = 0.01.
The minimun bigram perplexity is 246.56608232825454, lambda = 0.05000000000000001.

lambda = 0.01, pp1 = 276.26010688526037, pp2 = 258.11596193700206.
lambda = 0.020000000000000004, pp1 = 276.26010688526037, pp2 = 254.3185831400334.
lambda = 0.030000000000000006, pp1 = 276.26010688526037, pp2 = 252.24086590617787.
lambda = 0.04000000000000001, pp1 = 276.26010688526037, pp2 = 251.04060222146265.
lambda = 0.05000000000000001, pp1 = 276.26010688526037, pp2 = 250.3932406925007.
lambda = 0.06000000000000001, pp1 = 276.26010688526037, pp2 = 250.13548540285635.
lambda = 0.07, pp1 = 276.26010688526037, pp2 = 250.17265615037593.
lambda = 0.08, pp1 = 276.26010688526037, pp2 = 250.44472071081273.
lambda = 0.09000000000000001, pp1 = 276.26010688526037, pp2 = 250.91120274209453.
lambda = 0.1, pp1 = 276.26010688526037, pp2 = 251.54357322195136.
Using deceptive language model on truthful validation set:
The minimun unigram perplexity is 276.26010688526037, lambda = 0.01.
The minimun bigram perplexity is 250.13548540285635, lambda = 0.06000000000000001.

